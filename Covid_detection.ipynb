{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavaneethaCSR/Covid-Detection-using-CT-scan-Images/blob/main/Covid_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US6re9ag_4xw"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout,Input,Conv2D, MaxPooling2D,Activation,AveragePooling2D\n",
        "from tensorflow.keras import layers, models,regularizers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcXZGkqV6ujk",
        "outputId": "99486017-d7ce-4697-d5c2-fcb7b643dbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJq_H23WAZlH"
      },
      "outputs": [],
      "source": [
        "m, n = 224,224\n",
        "train_data='/content/drive/MyDrive/Colab Notebooks/Datasets/covid_dec/train'\n",
        "test_data='/content/drive/MyDrive/Colab Notebooks/Datasets/covid_dec/test'\n",
        "validation_data='/content/drive/MyDrive/Colab Notebooks/Datasets/covid_dec/validation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsicBhjRZL5J"
      },
      "source": [
        "# **PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIJVyZYy0YB9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def compress_image(image, quality=95):\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    _, compressed_image = cv2.imencode('.jpg', image, encode_param)\n",
        "    decoded_image = cv2.imdecode(compressed_image, 1)\n",
        "    return decoded_image\n",
        "\n",
        "def ela(image, quality=95):\n",
        "    compressed_image = compress_image(image, quality)\n",
        "    grayscale_image = cv2.cvtColor(compressed_image, cv2.COLOR_BGR2GRAY)\n",
        "    equalized_image = cv2.equalizeHist(grayscale_image)\n",
        "    ela_image = cv2.absdiff(grayscale_image, equalized_image)\n",
        "    return ela_image\n",
        "\n",
        "def preprocess_image(image_path, target_size=(224, 224), quality=95):\n",
        "    try:\n",
        "        original_image = cv2.imread(image_path)\n",
        "        if original_image is None:\n",
        "            raise ValueError(\"Failed to load image: {}\".format(image_path))\n",
        "\n",
        "        resized_image = cv2.resize(original_image, target_size)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        blurred_image = cv2.GaussianBlur(resized_image, (5, 5), 0)\n",
        "\n",
        "        #convert to grayscale\n",
        "        gray_image = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "        # Apply Sobel edge detection\n",
        "\n",
        "        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=5)\n",
        "        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=5)\n",
        "        edge_image = cv2.magnitude(sobel_x, sobel_y)\n",
        "\n",
        "\n",
        "        # Apply histogram equalization for contrast enhancement\n",
        "        equalized_image = cv2.equalizeHist(gray_image)\n",
        "\n",
        "        # Apply ELA\n",
        "        ela_result = ela(resized_image, quality)\n",
        "\n",
        "        # Combine preprocessing results\n",
        "        preprocessed_image = np.concatenate((blurred_image, np.expand_dims(edge_image, axis=-1), np.expand_dims(ela_result, axis=-1)), axis=-1)\n",
        "\n",
        "        # Normalize pixel values to the range [0, 1]\n",
        "        preprocessed_image = preprocessed_image / 255.0\n",
        "\n",
        "        return preprocessed_image\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image {}: {}\".format(image_path, e))\n",
        "        return None\n",
        "\n",
        "def preprocess_images_from_folders(data_dir, target_size=(224, 224), quality=95):\n",
        "    preprocessed_images = []\n",
        "    labels = []\n",
        "\n",
        "    for label in os.listdir(data_dir):\n",
        "        label_dir = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for filename in os.listdir(label_dir):\n",
        "                if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_path = os.path.join(label_dir, filename)\n",
        "                    preprocessed_image = preprocess_image(image_path, target_size, quality)\n",
        "                    if preprocessed_image is not None:\n",
        "                        preprocessed_images.append(preprocessed_image)\n",
        "                        labels.append(label)\n",
        "\n",
        "    return preprocessed_images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB3SKQvXA_bZ"
      },
      "outputs": [],
      "source": [
        "# Preprocess training images\n",
        "train_images, train_labels = preprocess_images_from_folders(train_data)\n",
        "\n",
        "# Preprocess validation images\n",
        "validation_images, validation_labels = preprocess_images_from_folders(validation_data)\n",
        "\n",
        "# Preprocess test images\n",
        "test_images, test_labels = preprocess_images_from_folders(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL6grBkOBh2E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert preprocessed images and labels to NumPy arrays\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "validation_images = np.array(validation_images)\n",
        "validation_labels = np.array(validation_labels)\n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ewZihFT33-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Instantiate LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels to numerical format\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "validation_labels_encoded = label_encoder.transform(validation_labels)\n",
        "\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0XBL7l7CZRj",
        "outputId": "f660b5f5-a084-44d4-9db3-b53386840cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7idD4MlVruJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers (reduced filters and added dropout)\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 5)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Dropout after first convolutional block\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))  # Dropout after second convolutional block\n",
        "\n",
        "# Flatten and dense layers (reduced neurons and added L2 regularization)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmxJ0sEXBwxr"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "import tensorflow as tf\n",
        "custom_optimizer = Adam(learning_rate=0.001, beta_1=0.9)\n",
        "model.compile(optimizer=custom_optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy',tf.keras.metrics.Precision(),\n",
        "                       tf.keras.metrics.Recall()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A59X9FueaYG9"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If the training loss continues to decrease while the validation loss increases, it's a sign of overfitting. However, if both losses decrease simultaneously or plateau together, it indicates that the model is not overfitting."
      ],
      "metadata": {
        "id": "ZOWT2ocDEaXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If the training accuracy continues to improve while the validation accuracy stagnates or decreases, it suggests overfitting. Balanced improvements or plateaus in both accuracies indicate no overfitting."
      ],
      "metadata": {
        "id": "7o0BKJAkEnEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TRAINING**"
      ],
      "metadata": {
        "id": "46C_Q2IgAnIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2gyifH5AKfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c348e5-5a22-4f2b-b992-e5953f8b35a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 12s 2s/step - loss: 1.2394 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 2.8551 - val_accuracy: 0.3500 - val_precision: 0.2857 - val_recall: 0.2000\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 11s 2s/step - loss: 1.1793 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 3.0159 - val_accuracy: 0.3000 - val_precision: 0.2500 - val_recall: 0.2000\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 10s 1s/step - loss: 1.1146 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 3.0959 - val_accuracy: 0.3000 - val_precision: 0.2500 - val_recall: 0.2000\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 12s 2s/step - loss: 1.0536 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 3.1340 - val_accuracy: 0.3500 - val_precision: 0.2857 - val_recall: 0.2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d4b282ff010>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.fit(train_images, train_labels_encoded, epochs=20, batch_size=16,validation_data=(validation_images, validation_labels_encoded),callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "vyh4dzh5AxaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5CBeCH8aD0e",
        "outputId": "b1c97036-9704-4e5f-82b0-0314455c2ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 423ms/step - loss: 1.7854 - accuracy: 0.7000 - precision: 0.7500 - recall: 0.6000\n"
          ]
        }
      ],
      "source": [
        "evaluation_results = model.evaluate(test_images,test_labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEpRmLcoSyho",
        "outputId": "c218d0a8-f365-4f3d-ee27-9ddba4a07fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 421ms/step\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label: non covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: covid\n",
            "Predicted Label: non covid Actual Label: covid\n",
            "Predicted Label:  covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label: non covid Actual Label: non_covid\n",
            "Predicted Label:  covid Actual Label: non_covid\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "predictions = model.predict(test_images)\n",
        "n=np.mean(predictions)\n",
        "# Get the actual labels\n",
        "actual_labels = test_labels\n",
        "\n",
        "# Print the predicted and actual labels\n",
        "for i in range(len(predictions)):\n",
        "    predicted_label = 'non covid' if predictions[i]>= n else ' covid'\n",
        "\n",
        "    print(\"Predicted Label:\", predicted_label, \"Actual Label:\", actual_labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVvYXplzpygp",
        "outputId": "0c8bdcab-c4ce-4d1e-dc17-de45d8055001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09088783]\n",
            "[0.02099293]\n",
            "[0.00068105]\n",
            "[0.05360046]\n",
            "[0.02397937]\n",
            "[0.85481423]\n",
            "[0.18815817]\n",
            "[0.02121178]\n",
            "[0.06538866]\n",
            "[0.641206]\n",
            "[0.06800812]\n",
            "[0.87539524]\n",
            "[0.9913794]\n",
            "[0.9596228]\n",
            "[0.8158789]\n",
            "[0.4798437]\n",
            "[0.98842895]\n",
            "[0.5333134]\n",
            "[0.49616247]\n",
            "[0.03882167]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(predictions)):\n",
        "  print(predictions[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXViqnpSrPVN"
      },
      "outputs": [],
      "source": [
        "pred_targets = np.where(predictions >= n, 1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7jUgSl_ql2U"
      },
      "outputs": [],
      "source": [
        "true_labels = label_encoder.fit_transform(actual_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPJzVeR7utMs"
      },
      "outputs": [],
      "source": [
        "confusion_test = confusion_matrix(true_labels, pred_targets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sABIisXC8vqV",
        "outputId": "45dfa28c-3df4-4aee-9027-f86261bf1dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8, 2],\n",
              "       [2, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VTOZjXzz0yHSOeHRZJuKcc7h4I7E3QR1",
      "authorship_tag": "ABX9TyOYFgK3A3ADLUodE26/BAMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}